# åˆå§‹åŒ–
## random initialization
ä½¿ç”¨`W_l = np.random.randn(layers_dim[l],layers_dim[l-1]) * const_number`æ¥å®Œæˆåˆå§‹åŒ–

## HEåˆå§‹åŒ–
ä¸€èˆ¬æ­é…RELUä½¿ç”¨å¯ä»¥è¾¾åˆ°æ¯”è¾ƒå¥½çš„æ•ˆæžœï¼Œåœ¨Lå±‚çš„weightsæƒé‡çŸ©é˜µåˆå§‹åŒ–æ—¶ï¼Œä¹˜ä»¥ä¸Šä¸€å±‚çš„å±‚æ•°ç›¸å…³å‚æ•°
`W_l = np.random.randn(layers_dim[l],layers_dim[l-1]) * np.sqrt(2/layers_dim[l-1])`  
å³ä¹˜ä»¥$\sqrt{\frac{2}{layers\_dim[l-1]}}$

# æ­£åˆ™åŒ–

## ä¸ä½¿ç”¨æ­£åˆ™åŒ–æ—¶
åœ¨è¿™ç§æƒ…å†µä¸‹æœ‰æ—¶ä¼šå‡ºçŽ°è¿‡æ‹Ÿåˆï¼Œå¦‚å›¾æ‰€ç¤º  
![](../../images/20201118151915.png)  
ä¸Šå›¾æ˜¯æ•°æ®åˆ†å¸ƒï¼Œä½¿ç”¨ç¥žç»ç½‘ç»œæ‹ŸåˆåŽå¾—åˆ°å¦‚ä¸‹æ•°æ®ï¼š
![](../../images/20201118152033.png)  
ä»–åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨çŽ°å¾ˆå¥½ï¼Œä½†æ˜¯åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨çŽ°ä¸å°½äººæ„ï¼Œå‡ºçŽ°äº†è¿‡æ‹ŸåˆçŽ°è±¡ï¼š  
![](../../images/20201118152232.png)

## L-2èŒƒå¼æ­£åˆ™åŒ–

Costå‡½æ•°:
![](../../images/20201118152424.png)

æ­£åˆ™åŒ–æ–¹æ³•ï¼š  
```Python
sum = 0
for l in range(1,layers):
    sum = sum + np.sum(np.square(W+str(l))
L2_regularization_cost = (1 / m) * (lambd / 2) * sum
```
åŒæ—¶åœ¨åå‘ä¼ æ’­ä¸­åŠ å…¥ä¿®æ­£é¡¹ï¼š  
$\frac{d}{dW}(\frac{\lambda}{2m}W^2) = \frac{\lambda}{m}W$
```Python
    dZ3 = A3 - Y
    
    ### START CODE HERE ### (approx. 1 line)
    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m * W3
    ### END CODE HERE ###
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m * W2
    ### END CODE HERE ###
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m * W1
    ### END CODE HERE ###
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)
```

æ­£åˆ™åŒ–åŽå¾—åˆ°ï¼š  
![](../../images/20201118155240.png)
![](../../images/20201118155250.png)

L2 regularization makes your decision boundary smoother. If  ðœ†  is too large, it is also possible to "oversmooth", resulting in a model with high bias.

å³ï¼š$\lambda$å¤ªå°ï¼Œç›¸å½“äºŽæ²¡æœ‰è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚å½“$\lambda$å¤ªå¤§æ—¶ï¼Œæ­£åˆ™åŒ–ä¿®æ­£è¿‡çŒ›ï¼Œå¯èƒ½ä¼šæ¬ æ‹Ÿåˆã€‚

# Dropout

***key:***
np.random.rand() è¾“å‡ºåœ¨[0,1)åŒºé—´çš„éšæœºæ•°
np.random.randn() è¾“å‡ºåœ¨æ­£æ€åˆ†å¸ƒå†…çš„éšæœºæ•°

Dropoutä¸€èˆ¬ç”¨åœ¨å­¦ä¹ å‚æ•°$n_x$ç»´åº¦æ¯”è¾ƒå¤§ï¼Œè€Œæ•°æ®é‡ä¸è¶³æ—¶(æ¯”å¦‚cvä¸­)

## å‰å‘ä¼ æ’­
æ­¥éª¤ï¼š  
![](../../images/20201118155745.png)  

```Python
def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):
    
    np.random.seed(1)
    
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]
    
    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    D1 = np.random.rand(A1.shape[0],A1.shape[1])
    D1 = D1 < keep_prob # å°†å°äºŽé˜ˆå€¼çš„ç½®1ï¼Œè¾¾åˆ°äº†è¯¥çŸ©é˜µå…ƒç´ 1-keep_pronçš„æ¦‚çŽ‡ä¸º0                                       
    A1 = A1 * D1                                  
    A1 = A1 / keep_prob # ä¿è¯æœŸæœ›å€¼ä¸å˜   

    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    D2 = np.random.rand(A2.shape[0],A2.shape[1])                                        
    D2 = D2 < keep_prob                                     
    A2 = A2 * D2                                        
    A2 = A2 / keep_prob                                
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    
    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)
    
    return A3, cache
```

## åŽå‘ä¼ æ’­
æ­¥éª¤ï¼š  
![](../../images/20201118161722.png)  

```Python
def backward_propagation_with_dropout(X, Y, cache, keep_prob):
    
    m = X.shape[1]
    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    dA2 = np.dot(W3.T, dZ3)
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T)
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
    dA1 = dA1 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
```

## æ•ˆæžœ
![](../../images/20201118162205.png)  
![](../../images/20201118162212.png)  
